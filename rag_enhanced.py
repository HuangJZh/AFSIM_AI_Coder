import os
import time
import logging
import re
import json
from typing import List, Dict, Any
from functools import lru_cache

import torch
from tqdm import tqdm

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM
)

from project_learner import AFSIMProjectLearner
from utils import ConfigManager, FileReader, setup_logging


class EnhancedRAGChatSystem:
    def __init__(self, 
                 project_root: str,
                 model_path: str = None,
                 documents_path: str = None,
                 embedding_model: str = None,
                 vector_db_dir: str = None):
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½é…ç½®
        self.config = ConfigManager()
        self.project_root = project_root
        self.documents_path = documents_path or project_root
        self.vector_db_dir = vector_db_dir or self.config.get('vector_db.persist_dir')
        self.model_path = model_path or self.config.get('model.path')
        self.embedding_model = embedding_model or self.config.get('embedding.model')
        
        self.logger.info("æ­£åœ¨åˆå§‹åŒ–AFSIMé¡¹ç›®å­¦ä¹ å™¨...")
        self.project_learner = AFSIMProjectLearner(project_root)
        self.project_learner.analyze_project_structure()
        
        self.logger.info("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
        self.embeddings = self._setup_embeddings()
        
        self.logger.info("æ­£åœ¨åŠ è½½å¤§è¯­è¨€æ¨¡å‹...")
        self.tokenizer, self.model = self._setup_llm()
        
        # æ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“
        self.vector_db = self.build_or_load_vector_db()
        self.enhanced_qa_chain = self.create_enhanced_qa_chain()
        
        self.conversation_history = []
        
        self.logger.info("EnhancedRAGChatSystem åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_embeddings(self):
        """è®¾ç½®åµŒå…¥æ¨¡å‹"""
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        try:
            return HuggingFaceBgeEmbeddings(
                model_name=self.embedding_model,
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True},
                query_instruction="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
            )
        except Exception as e:
            self.logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return HuggingFaceBgeEmbeddings(
                model_name=self.embedding_model,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True},
                query_instruction="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
            )
    
    def _setup_llm(self):
        """è®¾ç½®è¯­è¨€æ¨¡å‹"""
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True,
                padding_side='left'
            )
            
            # === å…³é”®ä¿®æ”¹ï¼šè®¾ç½®æˆªæ–­æ–¹å‘ä¸ºå·¦ä¾§ï¼Œä¿ç•™Promptæœ«å°¾çš„æŒ‡ä»¤ ===
            tokenizer.truncation_side = 'left' 
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # æ ¹æ®å¯ç”¨è®¾å¤‡é€‰æ‹©åŠ è½½æ–¹å¼
            if torch.cuda.is_available():
                model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    dtype=torch.float16,
                    device_map="cuda",
                    trust_remote_code=True,
                ).eval()
            
            return tokenizer, model
                
        except Exception as e:
            self.logger.error(f"åŠ è½½è¯­è¨€æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    @lru_cache(maxsize=100)
    def search_similar_documents_cached(self, query: str, top_k: int = None):
        """å¸¦ç¼“å­˜çš„æ–‡æ¡£æœç´¢"""
        if top_k is None:
            top_k = self.config.get('vector_db.search_k', 6)
        return self.vector_db.similarity_search(query, k=top_k)
    
    def build_or_load_vector_db(self):
        """æ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“"""
        if os.path.exists(self.vector_db_dir):
            self.logger.info("åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“...")
            try:
                vector_db = Chroma(
                    persist_directory=self.vector_db_dir,
                    embedding_function=self.embeddings
                )
                # éªŒè¯æ•°æ®åº“æ˜¯å¦æœ‰æ•ˆ
                if hasattr(vector_db, '_collection') and vector_db._collection.count() > 0:
                    self.logger.info("å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
                    return vector_db
                else:
                    self.logger.warning("å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œå°†é‡æ–°æ„å»º")
            except Exception as e:
                self.logger.error(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥ï¼Œå°†é‡æ–°æ„å»º: {e}")
        
        self.logger.info("æ„å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
        return self.build_knowledge_base()
    
    def build_knowledge_base(self):
        """æ„å»ºçŸ¥è¯†åº“"""
        self.logger.info("å¼€å§‹å¤„ç†æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“...")
        start_time = time.time()

        try:
            # æ”¶é›†æ‰€æœ‰å¯è¯»çš„æ–‡ä»¶
            all_txt_files = []
            for root, dirs, files in os.walk(self.documents_path):
                for file in files:
                    if file.endswith('.txt'):
                        file_path = os.path.join(root, file)
                        if not FileReader.should_skip_file(file_path):
                            all_txt_files.append(file_path)
            
            self.logger.info(f"æ‰¾åˆ° {len(all_txt_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...")
            
            documents = []
            skipped_files = 0
            
            # ä½¿ç”¨è¿›åº¦æ¡
            with tqdm(total=len(all_txt_files), desc="åŠ è½½æ–‡ä»¶") as pbar:
                for file_path in all_txt_files:
                    try:
                        content, encoding = FileReader.read_file_safely(file_path)
                        if content and content.strip():
                            from langchain_core.documents import Document
                            doc = Document(
                                page_content=content,
                                metadata={"source": file_path, "encoding": encoding}
                            )
                            documents.append(doc)
                        else:
                            skipped_files += 1
                    except Exception as e:
                        self.logger.warning(f"è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶: {file_path} - {e}")
                        skipped_files += 1
                    finally:
                        pbar.update(1)
                        pbar.set_postfix({"å½“å‰æ–‡ä»¶": os.path.basename(file_path)})
            
            if not documents:
                raise ValueError(f"åœ¨ {self.documents_path} ç›®å½•ä¸­æœªæ‰¾åˆ°å¯è¯»æ–‡æ¡£")
            
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæœ‰æ•ˆæ–‡æ¡£ (è·³è¿‡äº† {skipped_files} ä¸ªæ–‡ä»¶)")

            # åˆ†å‰²æ–‡æ¡£
            chunk_size = self.config.get('vector_db.chunk_size')
            chunk_overlap = self.config.get('vector_db.chunk_overlap')
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""]
            )
            
            with tqdm(total=len(documents), desc="åˆ†å‰²æ–‡æ¡£") as pbar:
                texts = []
                for doc in documents:
                    chunks = text_splitter.split_documents([doc])
                    texts.extend(chunks)
                    pbar.update(1)
            
            self.logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå¾—åˆ° {len(texts)} ä¸ªæ–‡æœ¬å—")

            # åˆ†æ‰¹æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.logger.info("åˆ†æ‰¹åˆ›å»ºå‘é‡æ•°æ®åº“...")
            vector_db = self._create_vector_db_in_batches(texts)
            
            end_time = time.time()
            self.logger.info(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œè€—æ—¶ {(end_time - start_time)/60:.2f} åˆ†é’Ÿ")
            return vector_db
            
        except Exception as e:
            self.logger.error(f"æ„å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            raise

    def _create_vector_db_in_batches(self, texts, batch_size=4000):
        """åˆ†æ‰¹åˆ›å»ºå‘é‡æ•°æ®åº“ä»¥é¿å…æ‰¹é‡å¤§å°é™åˆ¶"""
        from langchain_community.vectorstores import Chroma
        
        self.logger.info(f"å¼€å§‹åˆ†æ‰¹å¤„ç† {len(texts)} ä¸ªæ–‡æ¡£å—ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # ç¬¬ä¸€æ¬¡æ‰¹æ¬¡åˆ›å»ºæ•°æ®åº“
        first_batch = texts[:batch_size]
        self.logger.info(f"åˆ›å»ºåˆå§‹å‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {len(first_batch)} ä¸ªæ–‡æ¡£...")
        
        vector_db = Chroma.from_documents(
            documents=first_batch,
            embedding=self.embeddings,
            persist_directory=self.vector_db_dir
        )
        
        # å‰©ä½™æ‰¹æ¬¡é€æ­¥æ·»åŠ 
        remaining_texts = texts[batch_size:]
        if remaining_texts:
            self.logger.info(f"é€æ­¥æ·»åŠ å‰©ä½™ {len(remaining_texts)} ä¸ªæ–‡æ¡£...")
            
            for i in range(0, len(remaining_texts), batch_size):
                batch = remaining_texts[i:i + batch_size]
                self.logger.info(f"æ·»åŠ æ‰¹æ¬¡ {i//batch_size + 1}/{(len(remaining_texts)-1)//batch_size + 1}ï¼ŒåŒ…å« {len(batch)} ä¸ªæ–‡æ¡£")
                
                vector_db.add_documents(batch)
                
                # åŠæ—¶æ¸…ç†å†…å­˜
                del batch
        
        # æŒä¹…åŒ–æœ€ç»ˆæ•°æ®åº“
        vector_db.persist()
        self.logger.info("å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆå¹¶å·²æŒä¹…åŒ–")
        
        return vector_db
    
    def create_enhanced_qa_chain(self):
        """åˆ›å»ºå¢å¼ºçš„QAé“¾"""
        
        class CustomQwenLLM:
            def __init__(self, model, tokenizer, project_learner):
                self.model = model
                self.tokenizer = tokenizer
                self.project_learner = project_learner
                self.config = ConfigManager()
                self.logger = logging.getLogger(__name__)
            
            def __call__(self, prompt):
                try:
                    # ä½¿ç”¨é…ç½®ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…é»˜è®¤ä¸º4096 (ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡)
                    max_len = 32000 
                    
                    inputs = self.tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        truncation=True, 
                        max_length=max_len,
                        padding=True
                    )
                    inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=self.config.get('model.max_tokens'),
                            do_sample=True,
                            temperature=self.config.get('model.temperature'),
                            top_p=self.config.get('model.top_p'),
                            pad_token_id=self.tokenizer.eos_token_id,
                            repetition_penalty=self.config.get('model.repetition_penalty'),
                            use_cache=True
                        )
                    
                    response = self.tokenizer.decode(
                        outputs[0][inputs['input_ids'].shape[1]:], 
                        skip_special_tokens=True
                    ).strip()
                    
                    return response
                    
                except Exception as e:
                    self.logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
                    return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
        
        # åˆ›å»ºæ£€ç´¢å™¨
        search_k = self.config.get('vector_db.search_k')
        retriever = self.vector_db.as_retriever(search_kwargs={"k": search_k})
        
        class EnhancedCodeGenerationChain:
            def __init__(self, llm, retriever, project_learner):
                self.llm = llm
                self.retriever = retriever
                self.project_learner = project_learner
                self.logger = logging.getLogger(__name__)
            
            def run(self, query):
                try:
                    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
                    docs = self.retriever.get_relevant_documents(query)
                    
                    # è·å–é¡¹ç›®ä¸Šä¸‹æ–‡
                    project_context = self.project_learner.generate_context_prompt(query)
                    
                    # æ„å»ºå¢å¼ºçš„æç¤ºè¯
                    context = self._build_context_prompt(project_context, docs)
                    prompt = self._build_generation_prompt(context, query)
                    
                    # ç”Ÿæˆå›ç­”
                    response = self.llm(prompt)
                    
                    return {
                        "result": response,
                        "source_documents": docs,
                        "project_context": project_context
                    }
                    
                except Exception as e:
                    self.logger.error(f"è¿è¡Œå¢å¼ºä»£ç ç”Ÿæˆé“¾æ—¶å‡ºé”™: {e}")
                    return {
                        "result": f"ç”Ÿæˆä»£ç æ—¶å‡ºé”™: {str(e)}",
                        "source_documents": [],
                        "project_context": ""
                    }
            
            def _build_context_prompt(self, project_context, docs):
                """æ„å»ºä¸Šä¸‹æ–‡æç¤ºè¯"""
                context = " AFSIMé¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯:\n\n"
                context += project_context
                context += "\n\n ç›¸å…³ä»£ç ç¤ºä¾‹:\n\n"
                
                for i, doc in enumerate(docs, 1):
                    source_info = f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}" if hasattr(doc, 'metadata') else ""
                    context += f"ç¤ºä¾‹ {i} {source_info}:\n{doc.page_content[:500]}...\n{'='*50}\n"
                
                return context
            
            def _clean_query(self, query: str) -> str:
                """æ¸…ç†æŸ¥è¯¢ä¸­çš„é‡å¤å†…å®¹"""
                lines = query.split('\n')
                cleaned_lines = []
                required_keywords_seen = set()
                
                for line in lines:
                    line_clean = line.strip()
                    if not line_clean:
                        continue
                        
                    # æ£€æµ‹é‡å¤çš„"å¿…é¡»åŒ…å«"æ¨¡å¼
                    if "å¿…é¡»åŒ…å«" in line_clean:
                        key_content = re.sub(r'.*å¿…é¡»åŒ…å«', '', line_clean).strip()
                        if key_content and key_content not in required_keywords_seen:
                            cleaned_lines.append(line_clean)
                            required_keywords_seen.add(key_content)
                    else:
                        cleaned_lines.append(line_clean)
                
                # å¦‚æœæ¸…ç†åå†…å®¹å¤ªå°‘ï¼Œè¿”å›åŸå§‹æŸ¥è¯¢çš„é‡è¦éƒ¨åˆ†
                if len(cleaned_lines) < 2:
                    important_lines = []
                    for line in lines[:10]:
                        line_clean = line.strip()
                        if line_clean and "å¿…é¡»åŒ…å«" not in line_clean:
                            important_lines.append(line_clean)
                    return "\n".join(important_lines[:5])
                
                return "\n".join(cleaned_lines[:20])  # é™åˆ¶é•¿åº¦
            
            def _build_generation_prompt(self, context, query):
                """æ„å»ºç”Ÿæˆæç¤ºè¯"""
                cleaned_query = self._clean_query(query)

                return f"""ä½ æ˜¯ä¸€ä¸ªAFSIMä»£ç ç”Ÿæˆä¸“å®¶ï¼Œç†Ÿæ‚‰æ•´ä¸ªé¡¹ç›®ç»“æ„å’ŒåŸºç¡€åº“çš„ä½¿ç”¨ã€‚
{context}
 ç”¨æˆ·éœ€æ±‚: {query}
è¯·åŸºäºä»¥ä¸Šé¡¹ç›®ç»“æ„å’Œç›¸å…³ä»£ç ç¤ºä¾‹ï¼Œç›´æ¥ç”Ÿæˆå‡†ç¡®ã€å®Œæ•´çš„AFSIMä»£ç ã€‚
 ç¦æ­¢:
ä¸è¦æ·»åŠ è§£é‡Šæ€§æ–‡å­—
ä¸è¦é‡å¤ç›¸åŒçš„å†…å®¹
ä¸è¦è¾“å‡ºä¸å®Œæ•´çš„ä»£ç å—
è¯·ç”Ÿæˆå®Œæ•´çš„AFSIMä»£ç :"""
        
        return EnhancedCodeGenerationChain(
            llm=CustomQwenLLM(self.model, self.tokenizer, self.project_learner),
            retriever=retriever,
            project_learner=self.project_learner
        )
    
    # === æ–°å¢ä»£ç ä¿®å¤æ–¹æ³• (å¢å¼ºç‰ˆ) ===
    def generate_code_repair_response(self, code: str, error_log: str) -> Dict[str, Any]:
        """ç”Ÿæˆä»£ç ä¿®å¤å»ºè®®"""
        self.logger.info("æ­£åœ¨æ‰§è¡Œä»£ç ä¿®å¤...")
        try:
            # 1. æå–é”™è¯¯å…³é”®ä¿¡æ¯è¿›è¡ŒRAGæ£€ç´¢
            search_query = f"AFSIM code error: {error_log[:200]}"
            
            # ä½¿ç”¨æ£€ç´¢å™¨æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£
            docs = self.enhanced_qa_chain.retriever.get_relevant_documents(search_query)
            
            # 2. æ„å»ºä¿®å¤æç¤ºè¯
            context_docs = ""
            for i, doc in enumerate(docs[:3], 1):
                context_docs += f"å‚è€ƒæ–‡æ¡£ {i}:\n{doc.page_content[:400]}...\n\n"

            # è½¬ä¹‰å¤§æ‹¬å·ï¼Œé˜²æ­¢ f-string é”™è¯¯
            safe_code = code.replace("{", "{{").replace("}", "}}")
            safe_error = error_log.replace("{", "{{").replace("}", "}}")

            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªAFSIMä»¿çœŸè„šæœ¬ä»£ç ä¿®å¤ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹é”™è¯¯ä¿¡æ¯ä¿®å¤ä»£ç ã€‚

=== ğŸ“š å‚è€ƒè¯­æ³•æ–‡æ¡£ ===
{context_docs}

=== âŒ é”™è¯¯ä»£ç  ===
{safe_code}

=== âš ï¸ æŠ¥é”™ä¿¡æ¯ ===
{safe_error}

=== ğŸ› ï¸ ä¿®å¤ä»»åŠ¡ ===
1. åˆ†ææŠ¥é”™åŸå› ã€‚
2. æ ¹æ®å‚è€ƒæ–‡æ¡£ä¿®æ­£ä»£ç ä¸­çš„è¯­æ³•é”™è¯¯æˆ–é€»è¾‘é”™è¯¯ã€‚
3. **å¼ºåˆ¶ä½¿ç”¨æ ‡è®°åŒ…è£¹**ï¼šå°†ä¿®å¤åçš„å®Œæ•´ä»£ç åŒ…è£¹åœ¨ `<<<CODE_START>>>` å’Œ `<<<CODE_END>>>` ä¹‹é—´ã€‚
4. **ä¸¥ç¦è¾“å‡ºè§£é‡Š**ï¼šåªè¾“å‡ºä¿®å¤åçš„ä»£ç ï¼Œä¸è¦è¾“å‡ºâ€œæˆ‘å·²ä¿®å¤â€ã€â€œåŸå› å¦‚ä¸‹â€ç­‰åºŸè¯ã€‚

è¯·ç«‹å³è¾“å‡ºä¿®å¤åçš„ä»£ç ï¼š
"""
            # 3. è°ƒç”¨LLM
            response = self.enhanced_qa_chain.llm(prompt)
            
            # 4. æå–ä»£ç  (å¢å¼ºé²æ£’æ€§)
            code_match = re.search(r'<<<CODE_START>>>\s*(.*?)\s*<<<CODE_END>>>', response, re.DOTALL)
            if code_match:
                final_result = code_match.group(1)
            else:
                # å…œåº•1ï¼šå°è¯•æå–markdownå—
                code_block = re.search(r'```(?:afsim|txt|)\s*(.*?)\s*```', response, re.DOTALL)
                if code_block:
                    final_result = code_block.group(1)
                else:
                    # å…œåº•2ï¼šå¦‚æœæå–ä¸åˆ°ï¼Œè¿”å›åŸå§‹Responseï¼Œå¹¶æ‰“å°è­¦å‘Š
                    self.logger.warning("æœªæ‰¾åˆ°ä»£ç æ ‡è®°æˆ–Markdownå—ï¼Œè¿”å›åŸå§‹å›ç­”ã€‚")
                    final_result = response
            
            return {
                "result": final_result.strip(),
                "source_documents": docs
            }

        except Exception as e:
            self.logger.error(f"ä»£ç ä¿®å¤å¤±è´¥: {e}")
            return {
                "result": f"ä¿®å¤å¤±è´¥: {str(e)}",
                "source_documents": []
            }
            
    def generate_enhanced_response(self, query: str) -> Dict[str, Any]:
        """ç”Ÿæˆå¢å¼ºçš„å›ç­”"""
        try:
            # ä½¿ç”¨å¢å¼ºçš„QAé“¾ç”Ÿæˆå›ç­”
            result = self.enhanced_qa_chain.run(query)
            
            # æ›´æ–°å¯¹è¯å†å²
            self.conversation_history.append({
                'query': query,
                'response': result["result"],
                'sources': len(result["source_documents"]),
                'timestamp': time.time()
            })
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦
            if len(self.conversation_history) > 8:
                self.conversation_history = self.conversation_history[-6:]
            
            return result
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¢å¼ºå“åº”æ—¶å‡ºé”™: {e}")
            return {
                "result": f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}", 
                "source_documents": [],
                "project_context": ""
            }
    
    def get_vector_db_info(self):
        """è·å–å‘é‡æ•°æ®åº“ä¿¡æ¯"""
        if hasattr(self.vector_db, '_collection'):
            count = self.vector_db._collection.count()
            return f"å‘é‡æ•°æ®åº“åŒ…å« {count} ä¸ªæ–‡æ¡£ç‰‡æ®µ"
        return "å‘é‡æ•°æ®åº“ä¿¡æ¯ä¸å¯ç”¨"
    
    def get_project_info(self):
        """è·å–é¡¹ç›®ä¿¡æ¯"""
        return self.project_learner.get_project_summary()
    
    def search_project_files(self, keyword: str, max_results: int = 5):
        """åœ¨é¡¹ç›®ä¸­æœç´¢æ–‡ä»¶"""
        return self.project_learner.find_related_files(keyword, max_results)


class StageAwareRAGSystem:
    # ... (ä¿æŒåŸæ¥çš„ StageAwareRAGSystem ç±»å†…å®¹ä¸å˜)
    """ç®€åŒ–çš„é˜¶æ®µæ„ŸçŸ¥RAGç³»ç»Ÿ"""
    
    def __init__(self, project_learner: AFSIMProjectLearner, vector_db, embeddings, model, tokenizer):
        self.project_learner = project_learner
        self.vector_db = vector_db
        self.embeddings = embeddings
        self.model = model
        self.tokenizer = tokenizer
        self.config = ConfigManager()
        self.logger = logging.getLogger(__name__)
        
        # é˜¶æ®µæ£€ç´¢å™¨ç¼“å­˜
        self.stage_retrievers = {}
    
    def get_stage_retriever(self, stage_name: str):
        """è·å–é˜¶æ®µæ„ŸçŸ¥æ£€ç´¢å™¨"""
        if stage_name in self.stage_retrievers:
            return self.stage_retrievers[stage_name]
        
        # è·å–é˜¶æ®µç‰¹å®šçš„æ£€ç´¢å‚æ•°
        search_k = self.config.get(f'rag.stages.{stage_name}.search_k', 8)
        
        # åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨
        retriever = self.vector_db.as_retriever(
            search_kwargs={"k": search_k}
        )
        
        # åˆ›å»ºé˜¶æ®µæ„ŸçŸ¥çš„æ£€ç´¢å™¨åŒ…è£…å™¨
        stage_retriever = self._create_stage_retriever_wrapper(retriever, stage_name)
        self.stage_retrievers[stage_name] = stage_retriever
        
        return stage_retriever
    
    def _create_stage_retriever_wrapper(self, base_retriever, stage_name: str):
        """åˆ›å»ºé˜¶æ®µæ„ŸçŸ¥æ£€ç´¢å™¨åŒ…è£…å™¨"""
        
        def get_relevant_documents(query: str):
            """å¢å¼ºæŸ¥è¯¢å¹¶è¿‡æ»¤æ–‡æ¡£"""
            try:
                # å¢å¼ºæŸ¥è¯¢
                enhanced_query = self._enhance_query_for_stage(query, stage_name)
                
                # æ‰§è¡ŒåŸºç¡€æ£€ç´¢
                docs = base_retriever.get_relevant_documents(enhanced_query)
                
                # è¿‡æ»¤æ–‡æ¡£
                filtered_docs = self._filter_docs_by_stage(docs, stage_name)
                
                return filtered_docs[:8]  # é™åˆ¶è¿”å›æ•°é‡
                
            except Exception as e:
                self.logger.error(f"æ£€ç´¢æ–‡æ¡£å¤±è´¥: {e}")
                return []
        
        return get_relevant_documents
    
    def _enhance_query_for_stage(self, query: str, stage_name: str) -> str:
        """ä¸ºç‰¹å®šé˜¶æ®µå¢å¼ºæŸ¥è¯¢"""
        stage_keywords = {
            "platforms": ["platform", "aircraft", "vehicle"],
            "weapons": ["weapon", "missile", "launch"],
            "sensors": ["sensor", "radar", "detect"],
            "processors": ["processor", "algorithm", "control"],
            "scenarios": ["scenario", "mission", "environment"],
            "signatures": ["signature", "rcs", "emission"],
            "main_program": ["main", "include", "initialize"],
            "project_structure": ["project", "structure", "folder"]
        }
        
        keywords = stage_keywords.get(stage_name, [])
        enhanced = query
        
        if keywords:
            enhanced += " " + " ".join(keywords[:2])
        
        return enhanced
    
    def _filter_docs_by_stage(self, docs, stage_name: str):
        """æ ¹æ®é˜¶æ®µè¿‡æ»¤æ–‡æ¡£"""
        if not docs:
            return []
        
        stage_keywords_map = {
            "platforms": ["platform_type", "mover"],
            "weapons": ["weapon_type", "missile"],
            "sensors": ["sensor_type", "radar"],
            "processors": ["processor_type", "tasker"],
            "scenarios": ["scenario", "mission"],
            "signatures": ["signature", "rcs"],
            "main_program": ["main", "include"],
        }
        
        keywords = stage_keywords_map.get(stage_name, [])
        
        if not keywords:
            return docs
        
        filtered = []
        for doc in docs:
            doc_text = doc.page_content.lower()
            if any(keyword in doc_text for keyword in keywords):
                filtered.append(doc)
        
        # å¦‚æœè¿‡æ»¤åæ–‡æ¡£å¤ªå°‘ï¼Œè¿”å›åŸå§‹æ–‡æ¡£
        return filtered[:5] if len(filtered) >= 3 else docs[:5]


class EnhancedStageAwareRAGChatSystem(EnhancedRAGChatSystem):
    # ... (ä¿æŒåŸæ¥çš„ EnhancedStageAwareRAGChatSystem ç±»å†…å®¹ä¸å˜)
    """å¢å¼ºçš„é˜¶æ®µæ„ŸçŸ¥RAGèŠå¤©ç³»ç»Ÿ"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # åˆå§‹åŒ–é˜¶æ®µæ„ŸçŸ¥ç³»ç»Ÿ
        self.stage_aware_system = StageAwareRAGSystem(
            project_learner=self.project_learner,
            vector_db=self.vector_db,
            embeddings=self.embeddings,
            model=self.model,
            tokenizer=self.tokenizer
        )
        
        # é˜¶æ®µç‰¹å®šçš„QAé“¾ç¼“å­˜
        self.stage_qa_chains = {}
    
    def get_stage_aware_qa_chain(self, stage_name: str):
        """è·å–é˜¶æ®µæ„ŸçŸ¥çš„QAé“¾"""
        if stage_name in self.stage_qa_chains:
            return self.stage_qa_chains[stage_name]
        
        self.logger.info(f"ä¸ºé˜¶æ®µ {stage_name} æ„å»ºQAé“¾...")
        
        # è·å–é˜¶æ®µç‰¹å®šçš„æ£€ç´¢å™¨
        retriever = self.stage_aware_system.get_stage_retriever(stage_name)
        
        # åˆ›å»ºé˜¶æ®µç‰¹å®šçš„LLM
        class StageAwareLLM:
            def __init__(self, model, tokenizer, stage_name):
                self.model = model
                self.tokenizer = tokenizer
                self.stage_name = stage_name
                self.config = ConfigManager()
                self.logger = logging.getLogger(__name__)
            
            def __call__(self, prompt):
                try:
                    # é˜¶æ®µç‰¹å®šçš„ç”Ÿæˆå‚æ•°
                    stage_params = self.config.get(f'rag.stages.{self.stage_name}', {})
                    max_tokens = stage_params.get('max_tokens', self.config.get('model.max_tokens', 1024))
                    temperature = stage_params.get('temperature', self.config.get('model.temperature', 0.2))
                    
                    inputs = self.tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        truncation=True, 
                        max_length=32000,
                        padding=True
                    )
                    inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=max_tokens,
                            do_sample=True,
                            temperature=temperature,
                            top_p=self.config.get('model.top_p', 0.9),
                            pad_token_id=self.tokenizer.eos_token_id,
                            repetition_penalty=self.config.get('model.repetition_penalty', 1.1)
                        )
                    
                    response = self.tokenizer.decode(
                        outputs[0][inputs['input_ids'].shape[1]:], 
                        skip_special_tokens=True
                    ).strip()
                    
                    return response
                    
                except Exception as e:
                    self.logger.error(f"é˜¶æ®µ {self.stage_name} ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
                    return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
        
        # åˆ›å»ºé˜¶æ®µæ„ŸçŸ¥çš„QAé“¾
        class StageAwareQAChain:
            def __init__(self, llm, retriever, project_learner, stage_name):
                self.llm = llm
                self.retriever = retriever
                self.project_learner = project_learner
                self.stage_name = stage_name
                self.logger = logging.getLogger(__name__)
            
            def run(self, query: str, project_context: Dict = None) -> Dict[str, Any]:
                try:
                    # è·å–é˜¶æ®µç‰¹å®šçš„å­¦ä¹ ç»“æœ
                    stage_context = self.project_learner.generate_context_prompt(query)
                    
                    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
                    docs = self.retriever(query)
                    
                    # æ„å»ºå¢å¼ºæç¤ºè¯
                    prompt = self._build_stage_prompt(query, stage_context, docs, project_context)
                    
                    # ç”Ÿæˆå›ç­”
                    response = self.llm(prompt)
                    
                    return {
                        "result": response,
                        "stage_name": self.stage_name,
                        "source_documents": docs,
                        "stage_context": stage_context
                    }
                    
                except Exception as e:
                    self.logger.error(f"é˜¶æ®µ {self.stage_name} QAé“¾è¿è¡Œå¤±è´¥: {e}")
                    return {
                        "result": f"ç”Ÿæˆå¤±è´¥: {str(e)}",
                        "stage_name": self.stage_name,
                        "source_documents": [],
                        "stage_context": ""
                    }
            
            def _build_stage_prompt(self, query: str, stage_context: str, docs: List, project_context: Dict = None) -> str:
                """æ„å»ºé˜¶æ®µç‰¹å®šçš„æç¤ºè¯"""
                # æ–‡æ¡£å†…å®¹
                doc_content = ""
                for i, doc in enumerate(docs[:4], 1):
                    doc_content += f"ç¤ºä¾‹æ–‡æ¡£ {i}:\n{doc.page_content[:500]}...\n\n"
                
                # é¡¹ç›®ä¸Šä¸‹æ–‡
                proj_context = ""
                if project_context:
                    proj_context = f"\nå½“å‰é¡¹ç›®ä¸Šä¸‹æ–‡:\n{json.dumps(project_context, indent=2, ensure_ascii=False)}\n"
                
                # é˜¶æ®µç‰¹å®šçš„æŒ‡ä»¤
                stage_instructions = {
                    "project_structure": "ä½ éœ€è¦åˆ†æAFSIMé¡¹ç›®éœ€æ±‚å¹¶è§„åˆ’é¡¹ç›®ç»“æ„ã€‚è¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ªæ¸…æ™°çš„JSONæ ¼å¼ç»“æ„ï¼ŒåŒ…å«å¿…è¦çš„æ–‡ä»¶å¤¹å’Œæ–‡ä»¶è§„åˆ’ã€‚",
                    "platforms": "ä½ éœ€è¦ç”ŸæˆAFSIMå¹³å°å®šä¹‰ã€‚ç¡®ä¿åŒ…å«å®Œæ•´çš„å¹³å°ç±»å‹å®šä¹‰ã€ç‰©ç†å‚æ•°ã€ç»„ä»¶é…ç½®å’Œè¡Œä¸ºå®šä¹‰ã€‚",
                    "weapons": "ä½ éœ€è¦ç”ŸæˆAFSIMæ­¦å™¨å®šä¹‰ã€‚åŒ…æ‹¬æ­¦å™¨ç±»å‹ã€æ€§èƒ½å‚æ•°ã€åˆ¶å¯¼ç³»ç»Ÿå’Œæˆ˜æ–—éƒ¨é…ç½®ã€‚",
                    "sensors": "ä½ éœ€è¦ç”ŸæˆAFSIMä¼ æ„Ÿå™¨å®šä¹‰ã€‚åŒ…å«ä¼ æ„Ÿå™¨ç±»å‹ã€æ¢æµ‹å‚æ•°ã€å·¥ä½œæ¨¡å¼å’Œæ•°æ®è¾“å‡ºæ ¼å¼ã€‚",
                    "processors": "ä½ éœ€è¦ç”ŸæˆAFSIMå¤„ç†å™¨å®šä¹‰ã€‚åŒ…æ‹¬å¤„ç†å™¨ç±»å‹ã€è¾“å…¥è¾“å‡ºæ¥å£ã€å¤„ç†ç®—æ³•å’Œé…ç½®å‚æ•°ã€‚",
                    "scenarios": "ä½ éœ€è¦ç”ŸæˆAFSIMåœºæ™¯å®šä¹‰ã€‚åŒ…å«åœºæ™¯æè¿°ã€å¹³å°é…ç½®ã€ç¯å¢ƒè®¾ç½®å’Œäº‹ä»¶åºåˆ—ã€‚",
                    "signatures": "ä½ éœ€è¦ç”ŸæˆAFSIMç‰¹å¾ä¿¡å·å®šä¹‰ã€‚åŒ…æ‹¬ç‰¹å¾ç±»å‹ã€RCSå€¼ã€è§’åº¦ä¾èµ–æ€§å’Œè¾å°„ç‰¹æ€§ã€‚",
                    "main_program": "ä½ éœ€è¦ç”ŸæˆAFSIMä¸»ç¨‹åºã€‚åŒ…å«å¿…è¦çš„å¯¼å…¥ã€åˆå§‹åŒ–ã€äº‹ä»¶å¾ªç¯å’Œè¾“å‡ºé…ç½®ã€‚"
                }
                
                instruction = stage_instructions.get(self.stage_name, "è¯·æ ¹æ®éœ€æ±‚ç”Ÿæˆç›¸åº”çš„AFSIMä»£ç ã€‚")
                
                prompt = f"""{instruction}

é˜¶æ®µå­¦ä¹ æ€»ç»“:
{stage_context}

ç›¸å…³ä»£ç ç¤ºä¾‹:
{doc_content}
{proj_context}
ç”¨æˆ·éœ€æ±‚: {query}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆå®Œæ•´çš„{self.stage_name}é˜¶æ®µä»£ç ã€‚
è¾“å‡ºè¦æ±‚:
1. åªè¾“å‡ºAFSIMä»£ç ï¼Œä¸æ·»åŠ é¢å¤–è§£é‡Š
2. ç¡®ä¿ä»£ç å®Œæ•´æ€§å’Œæ­£ç¡®æ€§
3. éµå¾ªç¤ºä¾‹ä¸­çš„æœ€ä½³å®è·µ

ç”Ÿæˆä»£ç :"""
                
                return prompt
        
        # åˆ›å»ºLLMå®ä¾‹
        llm = StageAwareLLM(self.model, self.tokenizer, stage_name)
        
        # åˆ›å»ºQAé“¾
        qa_chain = StageAwareQAChain(llm, retriever, self.project_learner, stage_name)
        
        self.stage_qa_chains[stage_name] = qa_chain
        return qa_chain
    
    def generate_stage_response(self, stage_name: str, query: str, project_context: Dict = None) -> Dict[str, Any]:
        """ç”Ÿæˆé˜¶æ®µç‰¹å®šçš„å“åº”"""
        try:
            # è·å–é˜¶æ®µæ„ŸçŸ¥çš„QAé“¾
            qa_chain = self.get_stage_aware_qa_chain(stage_name)
            
            # æ‰§è¡Œç”Ÿæˆ
            result = qa_chain.run(query, project_context)
            
            # è®°å½•åˆ°å†å²
            self.conversation_history.append({
                'stage': stage_name,
                'query': query,
                'response': result["result"],
                'timestamp': time.time()
            })
            
            return result
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆé˜¶æ®µ {stage_name} å“åº”å¤±è´¥: {e}")
            return {
                "result": f"ç”Ÿæˆå¤±è´¥: {str(e)}",
                "stage_name": stage_name,
                "error": str(e)
            }
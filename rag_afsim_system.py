import os
import torch
from typing import List, Dict, Any, Optional
import numpy as np
from chromadb import PersistentClient
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging
from pathlib import Path
import traceback

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
try:
    from utils import ConfigManager
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œåˆ›å»ºç®€å•çš„é…ç½®ç®¡ç†å™¨
    class SimpleConfigManager:
        def __init__(self):
            self.config = {}
        def get(self, key, default=None):
            return default
        def get_int(self, key, default=0):
            return default
        def get_float(self, key, default=0.0):
            return default
        def get_bool(self, key, default=False):
            return default
    
    ConfigManager = SimpleConfigManager

logger = logging.getLogger(__name__)

class AFSIMRAGSystem:
    def __init__(self, 
                 model_path: Optional[str] = None,
                 embedding_model: Optional[str] = None,
                 chroma_db_path: Optional[str] = None):
        """
        åˆå§‹åŒ–AFSIM RAGç³»ç»Ÿ
        """
        logger.info("æ­£åœ¨åˆå§‹åŒ–AFSIM RAGç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config = ConfigManager()
        
        # ä½¿ç”¨é…ç½®å€¼æˆ–å‚æ•°å€¼
        self.model_path = model_path or self.config.get('model.path')
        self.embedding_model_name = embedding_model or self.config.get('embedding.model_name')
        self.chroma_db_path = chroma_db_path or self.config.get('database.chroma_path')
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not self.model_path or not os.path.exists(self.model_path):
            logger.warning(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            logger.info("å°†å°è¯•ä»HuggingFaceä¸‹è½½æˆ–ä½¿ç”¨é»˜è®¤è·¯å¾„")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_embedding_model()
        self._init_vector_db()
        self._init_llm()
        
        logger.info("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        
    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        logger.info(f"åŠ è½½åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
        try:
            self.embedding_model = SentenceTransformer(
                self.embedding_model_name,
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
            
            # ä»é…ç½®è·å–åµŒå…¥å‚æ•°
            self.normalize_embeddings = self.config.get_bool('embedding.normalize_embeddings', True)
            self.embedding_batch_size = self.config.get_int('embedding.batch_size', 32)
            
            logger.info(f"åµŒå…¥ç»´åº¦: {self.embedding_dim}")
        except Exception as e:
            logger.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _init_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        logger.info(f"åˆå§‹åŒ–Chromaæ•°æ®åº“: {self.chroma_db_path}")
        
        try:
            # åˆ›å»ºæ•°æ®åº“ç›®å½•å¦‚æœä¸å­˜åœ¨
            os.makedirs(self.chroma_db_path, exist_ok=True)
            
            db_settings = self.config.get('database.settings', {})
            self.client = PersistentClient(
                path=self.chroma_db_path,
                settings=Settings(
                    anonymized_telemetry=db_settings.get('anonymized_telemetry', False),
                    is_persistent=True
                )
            )
            
            # åˆ›å»ºæˆ–è·å–é›†åˆ
            collection_name = self.config.get('vector_db.collection_name', 'afsim_tutorials')
            self.collection = self.client.get_or_create_collection(
                name=collection_name,
                metadata={
                    "description": "AFSIMæ•™ç¨‹æ–‡æ¡£å‘é‡å­˜å‚¨",
                    "model": self.embedding_model_name
                }
            )
            
            doc_count = self.collection.count()
            logger.info(f"æ•°æ®åº“æ–‡æ¡£æ•°é‡: {doc_count}")
            
        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_llm(self):
        """åˆå§‹åŒ–Qwen3-4Bæ¨¡å‹"""
        logger.info(f"åŠ è½½Qwen3-4Bæ¨¡å‹: {self.model_path}")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                padding_side="left"  # å¯¹äºç”Ÿæˆä»»åŠ¡ï¼Œpaddingåº”è¯¥åœ¨å·¦è¾¹
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                if self.tokenizer.eos_token is not None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                else:
                    self.tokenizer.pad_token = self.tokenizer.sep_token or "<pad>"
            
            # ä»é…ç½®è·å–åŠ è½½å‚æ•°
            dtype_str = self.config.get('system.dtype', 'float16')
            load_in_4bit = self.config.get_bool('system.load_in_4bit', True)
            use_quantization = self.config.get_bool('system.use_quantization', True)
            device_map = self.config.get('system.device_map', 'auto')
            
            dtype = getattr(torch, dtype_str) if hasattr(torch, dtype_str) else torch.float16
            
            # æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "trust_remote_code": True,
                "dtype": dtype,
                "device_map": device_map,
            }
            
            # å°è¯•ä½¿ç”¨é‡åŒ–åŠ è½½
            if use_quantization and load_in_4bit:
                try:
                    from transformers import BitsAndBytesConfig
                    bnb_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=dtype,
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4"
                    )
                    model_kwargs["quantization_config"] = bnb_config
                    logger.info("ä½¿ç”¨4-bité‡åŒ–é…ç½®")
                except ImportError:
                    logger.warning("æœªå®‰è£…bitsandbytesï¼Œæ— æ³•ä½¿ç”¨4-bité‡åŒ–")
                    load_in_4bit = False
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **model_kwargs
            )
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # ä»é…ç½®è·å–ç”Ÿæˆå‚æ•°
            generation_config = self.config.get('model.generation', {})
            self.generation_config = {
                "max_new_tokens": self.config.get_int('model.generation.max_new_tokens', 1024),
                "temperature": self.config.get_float('model.generation.temperature', 0.3),
                "top_p": self.config.get_float('model.generation.top_p', 0.9),
                "do_sample": generation_config.get('do_sample', True),
                "repetition_penalty": self.config.get_float('model.generation.repetition_penalty', 1.1),
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True
            }
            
            logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
            logger.info(f"ç”Ÿæˆé…ç½®: {self.generation_config}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_documents_from_folder(self, folder_path: Optional[str] = None) -> bool:
        """
        ä»æ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰.mdæ–‡ä»¶åˆ°å‘é‡æ•°æ®åº“
        """
        if folder_path is None:
            folder_path = self.config.get('paths.tutorials_folder', 'tutorials')
        
        logger.info(f"å¼€å§‹æ‰«ææ–‡ä»¶å¤¹: {folder_path}")
        
        if not os.path.exists(folder_path):
            logger.error(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return False
        
        if not os.path.isdir(folder_path):
            logger.error(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}")
            return False
        
        try:
            # ä»é…ç½®è·å–æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
            supported_extensions = self.config.get('document.supported_extensions', ['.md', '.txt'])
            
            # æ‰«ææ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
            supported_files = []
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    if any(file.endswith(ext) for ext in supported_extensions):
                        full_path = os.path.join(root, file)
                        supported_files.append(full_path)
            
            logger.info(f"æ‰¾åˆ° {len(supported_files)} ä¸ªæ”¯æŒçš„æ–‡ä»¶")
            
            if not supported_files:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶")
                return False
            
            # æ¸…ç©ºç°æœ‰é›†åˆ - ä¿®å¤ï¼šä¸èƒ½ä½¿ç”¨ç©ºçš„whereæ¡ä»¶
            self._clear_collection()
            
            documents = []
            metadatas = []
            ids = []
            
            # ä»é…ç½®è·å–åˆ†å—å‚æ•°
            chunk_size = self.config.get_int('vector_db.chunk_size', 1500)
            chunk_overlap = self.config.get_int('vector_db.chunk_overlap', 250)
            max_file_size = self.config.get_int('document.max_file_size_mb', 10) * 1024 * 1024
            
            # è¯»å–æ¯ä¸ªæ–‡ä»¶
            for file_idx, file_path in enumerate(supported_files, 1):
                try:
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    file_size = os.path.getsize(file_path)
                    if file_size > max_file_size:
                        logger.warning(f"æ–‡ä»¶è¿‡å¤§è·³è¿‡: {os.path.basename(file_path)} ({file_size/1024/1024:.1f}MB)")
                        continue
                    
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        doc_content = f.read()
                    
                    if not doc_content.strip():
                        logger.warning(f"æ–‡ä»¶å†…å®¹ä¸ºç©º: {os.path.basename(file_path)}")
                        continue
                    
                    # åˆ†å‰²æ–‡æ¡£
                    chunks = self._split_into_chunks(
                        doc_content, 
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    
                    for chunk_idx, chunk in enumerate(chunks):
                        if chunk.strip():  # è·³è¿‡ç©ºå—
                            doc_id = f"{os.path.basename(file_path)}_{file_idx}_{chunk_idx}"
                            documents.append(chunk)
                            metadatas.append({
                                "source": file_path,
                                "chunk": chunk_idx,
                                "filename": os.path.basename(file_path),
                                "filepath": file_path,
                                "total_chunks": len(chunks)
                            })
                            ids.append(doc_id)
                    
                    logger.info(f"å·²åŠ è½½: {os.path.basename(file_path)} ({len(chunks)} ä¸ªå—)")
                    
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    traceback.print_exc()
            
            # æ‰¹é‡åµŒå…¥å¹¶å­˜å‚¨
            if documents:
                logger.info(f"æ­£åœ¨ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£å—çš„å‘é‡...")
                
                # åˆ†æ‰¹å¤„ç†
                batch_size = self.embedding_batch_size
                total_batches = (len(documents) + batch_size - 1) // batch_size
                
                for batch_idx in range(0, len(documents), batch_size):
                    end_idx = min(batch_idx + batch_size, len(documents))
                    batch_docs = documents[batch_idx:end_idx]
                    
                    # ç”ŸæˆåµŒå…¥
                    embeddings = self.embedding_model.encode(
                        batch_docs,
                        normalize_embeddings=self.normalize_embeddings,
                        show_progress_bar=False,
                        convert_to_numpy=True
                    )
                    
                    # å­˜å‚¨åˆ°æ•°æ®åº“
                    self.collection.add(
                        embeddings=embeddings.tolist(),
                        documents=batch_docs,
                        metadatas=metadatas[batch_idx:end_idx],
                        ids=ids[batch_idx:end_idx]
                    )
                    
                    logger.info(f"  å·²å¤„ç†æ‰¹æ¬¡ {batch_idx//batch_size + 1}/{total_batches} ({end_idx}/{len(documents)})")
                
                logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£å—")
                return True
            else:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£å†…å®¹")
                return False
                
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def _clear_collection(self):
        """æ¸…ç©ºé›†åˆä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        try:
            # å…ˆå°è¯•è·å–æ‰€æœ‰æ–‡æ¡£ID
            try:
                # å°è¯•è·å–æ‰€æœ‰æ–‡æ¡£
                results = self.collection.get()
                if results and 'ids' in results and results['ids']:
                    # å¦‚æœæœ‰æ–‡æ¡£ï¼Œä½¿ç”¨idsåˆ é™¤
                    self.collection.delete(ids=results['ids'])
                    logger.info(f"æ¸…ç©ºäº† {len(results['ids'])} ä¸ªæ–‡æ¡£")
                else:
                    logger.info("é›†åˆä¸ºç©ºï¼Œæ— éœ€æ¸…ç©º")
            except Exception as e:
                logger.warning(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
                
                # å¤‡ç”¨æ–¹æ³•ï¼šå°è¯•ä½¿ç”¨whereæ¡ä»¶åˆ é™¤
                try:
                    # å°è¯•åˆ é™¤æ‰€æœ‰æ–‡æ¡£
                    self.collection.delete(where={"filename": {"$ne": ""}})
                    logger.info("ä½¿ç”¨whereæ¡ä»¶æ¸…ç©ºé›†åˆ")
                except Exception as e2:
                    logger.warning(f"ä½¿ç”¨whereæ¡ä»¶åˆ é™¤å¤±è´¥: {e2}")
                    
                    # æœ€åæ‰‹æ®µï¼šåˆ é™¤å¹¶é‡æ–°åˆ›å»ºé›†åˆ
                    collection_name = self.collection.name
                    self.client.delete_collection(collection_name)
                    logger.info(f"åˆ é™¤äº†é›†åˆ: {collection_name}")
                    
                    # é‡æ–°åˆ›å»ºé›†åˆ
                    self.collection = self.client.get_or_create_collection(
                        name=collection_name,
                        metadata={
                            "description": "AFSIMæ•™ç¨‹æ–‡æ¡£å‘é‡å­˜å‚¨",
                            "model": self.embedding_model_name
                        }
                    )
                    logger.info(f"é‡æ–°åˆ›å»ºäº†é›†åˆ: {collection_name}")
                    
        except Exception as e:
            logger.error(f"æ¸…ç©ºé›†åˆå¤±è´¥: {e}")
            traceback.print_exc()
    
    def load_documents_from_list(self, file_list_path: str, base_dir: str = ".") -> bool:
        """
        ä»æ–‡ä»¶åˆ—è¡¨åŠ è½½æ–‡æ¡£ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        """
        logger.info(f"ä»æ–‡ä»¶åˆ—è¡¨åŠ è½½æ–‡æ¡£: {file_list_path}")
        
        if not os.path.exists(file_list_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_list_path}")
            return False
        
        try:
            with open(file_list_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # è¿‡æ»¤ç©ºè¡Œå’Œæ³¨é‡Š
            file_paths = []
            for line in lines:
                line = line.strip()
                if line and not line.startswith('#'):
                    file_paths.append(line)
            
            logger.info(f"æ–‡ä»¶åˆ—è¡¨ä¸­æœ‰ {len(file_paths)} ä¸ªæ–‡ä»¶")
            
            # æ¸…ç©ºç°æœ‰é›†åˆ
            self._clear_collection()
            
            documents = []
            metadatas = []
            ids = []
            
            chunk_size = self.config.get_int('vector_db.chunk_size', 1500)
            chunk_overlap = self.config.get_int('vector_db.chunk_overlap', 250)
            
            for file_idx, line in enumerate(file_paths, 1):
                try:
                    # æ¸…ç†è·¯å¾„
                    file_path = line.replace('D:.\\', '').replace('D:.', '').strip()
                    file_path = file_path.replace('\\', '/')
                    
                    # æ·»åŠ åŸºç¡€ç›®å½•
                    if not os.path.isabs(file_path):
                        file_path = os.path.join(base_dir, file_path)
                    
                    if os.path.exists(file_path):
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            doc_content = f.read()
                        
                        chunks = self._split_into_chunks(
                            doc_content,
                            chunk_size=chunk_size,
                            chunk_overlap=chunk_overlap
                        )
                        
                        for chunk_idx, chunk in enumerate(chunks):
                            if chunk.strip():
                                doc_id = f"{os.path.basename(file_path)}_{file_idx}_{chunk_idx}"
                                documents.append(chunk)
                                metadatas.append({
                                    "source": file_path,
                                    "chunk": chunk_idx,
                                    "filename": os.path.basename(file_path),
                                    "filepath": file_path,
                                    "total_chunks": len(chunks)
                                })
                                ids.append(doc_id)
                        
                        logger.info(f"å·²åŠ è½½: {os.path.basename(file_path)} ({len(chunks)} ä¸ªå—)")
                        
                    else:
                        logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                        
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {line}: {e}")
                    traceback.print_exc()
            
            if documents:
                # åˆ†æ‰¹åµŒå…¥
                batch_size = self.embedding_batch_size
                
                for i in range(0, len(documents), batch_size):
                    end_idx = min(i + batch_size, len(documents))
                    batch_docs = documents[i:end_idx]
                    
                    embeddings = self.embedding_model.encode(
                        batch_docs,
                        normalize_embeddings=self.normalize_embeddings,
                        show_progress_bar=False,
                        convert_to_numpy=True
                    )
                    
                    self.collection.add(
                        embeddings=embeddings.tolist(),
                        documents=batch_docs,
                        metadatas=metadatas[i:end_idx],
                        ids=ids[i:end_idx]
                    )
                
                logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£å—")
                return True
            else:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£å†…å®¹")
                return False
                
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def _split_into_chunks(self, text: str, chunk_size: int = 1500, chunk_overlap: int = 250) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆé‡å çš„å—"""
        if not text.strip():
            return []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for para in paragraphs:
            para_length = len(para)
            
            # å¦‚æœæ®µè½æœ¬èº«è¶…è¿‡chunk_sizeï¼Œéœ€è¦åˆ†å‰²æ®µè½
            if para_length > chunk_size:
                # å…ˆæ·»åŠ å½“å‰å—
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                    current_chunk = []
                    current_length = 0
                
                # åˆ†å‰²å¤§æ®µè½
                words = para.split()
                temp_chunk = []
                temp_length = 0
                
                for word in words:
                    word_length = len(word) + 1  # åŠ 1æ˜¯ä¸ºäº†ç©ºæ ¼
                    if temp_length + word_length <= chunk_size:
                        temp_chunk.append(word)
                        temp_length += word_length
                    else:
                        if temp_chunk:
                            chunks.append(' '.join(temp_chunk))
                        
                        # è®¾ç½®é‡å 
                        overlap_words = temp_chunk[-chunk_overlap//5:] if chunk_overlap > 0 else []
                        temp_chunk = overlap_words + [word]
                        temp_length = sum(len(w) + 1 for w in temp_chunk)
                
                if temp_chunk:
                    chunks.append(' '.join(temp_chunk))
            
            # æ­£å¸¸æƒ…å†µï¼šæ®µè½é€‚åˆå½“å‰å—
            elif current_length + para_length <= chunk_size:
                current_chunk.append(para)
                current_length += para_length + 2  # åŠ 2æ˜¯ä¸ºäº†\n\n
            else:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å®ƒ
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                
                # åˆ›å»ºæ–°å—ï¼Œè€ƒè™‘é‡å 
                if chunk_overlap > 0 and current_chunk:
                    # è®¡ç®—é‡å å†…å®¹
                    overlap_text = '\n\n'.join(current_chunk)
                    overlap_words = overlap_text.split()
                    overlap_size = min(len(overlap_words), chunk_overlap // 5)
                    overlap_content = ' '.join(overlap_words[-overlap_size:]) if overlap_size > 0 else ""
                    
                    current_chunk = [overlap_content, para] if overlap_content else [para]
                    current_length = len('\n\n'.join(current_chunk))
                else:
                    current_chunk = [para]
                    current_length = para_length
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    
    def retrieve_relevant_docs(self, query: str, n_results: int = 5) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        doc_count = self.collection.count()
        if doc_count == 0:
            logger.warning("å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆåŠ è½½æ–‡æ¡£")
            return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode(
                query,
                normalize_embeddings=True,
                show_progress_bar=False
            ).tolist()
            
            # æ£€ç´¢ï¼Œå¢åŠ ç»“æœæ•°ä»¥æé«˜å¬å›ç‡
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=min(n_results * 2, doc_count),
                include=["documents", "metadatas", "distances"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            retrieved_docs = []
            if results['documents'] and len(results['documents'][0]) > 0:
                for i, doc in enumerate(results['documents'][0]):
                    retrieved_docs.append({
                        'content': doc,
                        'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                        'distance': results['distances'][0][i] if results['distances'] else None,
                        'score': 1.0 - (results['distances'][0][i] if results['distances'] else 1.0)
                    })
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            retrieved_docs.sort(key=lambda x: x['score'], reverse=True)
            
            # åªè¿”å›å‰n_resultsä¸ª
            return retrieved_docs[:n_results]
            
        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            traceback.print_exc()
            return []
    
    def format_prompt(self, query: str, retrieved_docs: List[Dict]) -> str:
        """æ ¼å¼åŒ–æç¤ºè¯"""
        system_prompt = """ä½ æ˜¯AFSIMä¸“å®¶åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·ç¼–å†™AFSIMä»¿çœŸä»£ç ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„æ•™ç¨‹å†…å®¹å’Œä¸Šä¸‹æ–‡ï¼Œ**åªè¦ç”Ÿæˆå‡†ç¡®ã€å®Œæ•´ã€å¯è¿è¡Œçš„AFSIMä»£ç **ã€‚

è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. åªè¾“å‡ºæœ‰æ•ˆçš„AFSIMä»£ç 
2. ä¿æŒä»£ç ç®€æ´é«˜æ•ˆ

AFSIMä»£ç ï¼š"""

        if not retrieved_docs:
            context = "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ•™ç¨‹å†…å®¹ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ã€‚"
        else:
            context = "ä»¥ä¸‹æ˜¯ç›¸å…³çš„AFSIMæ•™ç¨‹å†…å®¹ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰ï¼š\n\n"
            for i, doc in enumerate(retrieved_docs, 1):
                filename = doc['metadata'].get('filename', 'æœªçŸ¥æ–‡ä»¶')
                relevance = f"ç›¸å…³æ€§: {doc['score']:.2%}" if doc.get('score') else ""
                context += f"ã€æ–‡æ¡£{i}ã€‘{filename} {relevance}\n"
                context += f"{doc['content']}\n\n"
        
        user_query = f"ç”¨æˆ·é—®é¢˜ï¼š{query}"
        
        prompt = f"{system_prompt}\n\n{context}\n{user_query}\n\nè¯·ç”ŸæˆAFSIMä»£ç ï¼š"
        
        return prompt
    
    def generate_response(self, query: str) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”"""
        logger.info(f"å¤„ç†æŸ¥è¯¢: {query[:100]}...")
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve_relevant_docs(query, n_results=4)
        
        if retrieved_docs:
            logger.info(f"æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            for doc in retrieved_docs[:2]:
                logger.debug(f"æ–‡æ¡£: {doc['metadata'].get('filename', 'æœªçŸ¥')}, åˆ†æ•°: {doc.get('score', 0):.3f}")
        
        # æ„å»ºæç¤º
        prompt = self.format_prompt(query, retrieved_docs)
        
        try:
            # è®¡ç®—æœ€å¤§è¾“å…¥é•¿åº¦
            max_tokens = self.config.get_int('model.max_tokens', 4096)
            max_new_tokens = self.generation_config.get('max_new_tokens', 1024)
            max_input_tokens = max_tokens - max_new_tokens - 100  # ç•™å‡ºç¼“å†²
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=max_input_tokens,
                padding=True
            )
            
            # ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
            device = self.model.device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **self.generation_config
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                skip_special_tokens=True
            )
            
            # æ¸…ç†å“åº”
            response = self._clean_response(response)
            
            # æå–æ¥æºä¿¡æ¯
            sources = []
            for doc in retrieved_docs:
                filename = doc['metadata'].get('filename')
                if filename and filename not in sources:
                    sources.append(filename)
            
            logger.info(f"å›ç­”ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(response)} å­—ç¬¦ï¼Œæ¥æº: {len(sources)} ä¸ªæ–‡ä»¶")
            
            return {
                "response": response,
                "sources": sources,
                "raw_docs": retrieved_docs[:3]  # åªä¿ç•™å‰3ä¸ªåŸå§‹æ–‡æ¡£
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            traceback.print_exc()
            return {
                "response": f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}\nè¯·æ£€æŸ¥æ¨¡å‹é…ç½®æˆ–å°è¯•é‡æ–°åˆå§‹åŒ–ç³»ç»Ÿã€‚",
                "sources": [],
                "raw_docs": []
            }
    
    def _clean_response(self, text: str) -> str:
        """æ¸…ç†å“åº”æ–‡æœ¬"""
        if not text:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç”Ÿæˆä»»ä½•å†…å®¹ã€‚è¯·å°è¯•é‡æ–°æé—®æˆ–æ£€æŸ¥ç³»ç»Ÿé…ç½®ã€‚"
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        lines = text.strip().split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.rstrip()
            if line:
                cleaned_lines.append(line)
        
        # é‡æ–°ç»„åˆï¼Œç¡®ä¿ä»£ç æ ¼å¼
        cleaned_text = '\n'.join(cleaned_lines)
        
        # å¦‚æœä»¥ä»£ç å¸¸è§æ ‡ç‚¹ç»“å°¾ï¼Œç¡®ä¿æœ‰æ¢è¡Œ
        code_endings = ['}', ';', ']', ')']
        if cleaned_text and cleaned_text[-1] in code_endings:
            cleaned_text += '\n'
        
        # é™åˆ¶æœ€å¤§é•¿åº¦
        max_length = self.generation_config.get('max_new_tokens', 1024) * 4  # ç²—ç•¥ä¼°è®¡
        if len(cleaned_text) > max_length:
            cleaned_text = cleaned_text[:max_length] + "\n\n...(å›ç­”è¿‡é•¿ï¼Œå·²æˆªæ–­)"
        
        return cleaned_text
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©"""
        print("\n" + "="*60)
        print("AFSIM RAG ç³»ç»Ÿ - äº¤äº’æ¨¡å¼")
        print("="*60)
        print("å‘½ä»¤:")
        print("  'exit' æˆ– 'quit' - é€€å‡º")
        print("  'clear' - æ¸…ç©ºä¸Šä¸‹æ–‡")
        print("  'sources' - æ˜¾ç¤ºå½“å‰æ¥æº")
        print("  'reload' - é‡æ–°åŠ è½½æ–‡æ¡£")
        print("  'stats' - æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡")
        print("="*60)
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ ç”¨æˆ·: ").strip()
                
                if user_input.lower() in ['exit', 'quit', 'q']:
                    print("å†è§ï¼")
                    break
                elif user_input.lower() == 'clear':
                    print("ä¸Šä¸‹æ–‡å·²æ¸…ç©º")
                    continue
                elif user_input.lower() == 'sources':
                    doc_count = self.collection.count()
                    print(f"æ•°æ®åº“ä¸­æœ‰ {doc_count} ä¸ªæ–‡æ¡£å—")
                    continue
                elif user_input.lower() == 'reload':
                    folder = self.config.get('paths.tutorials_folder', 'tutorials')
                    print(f"é‡æ–°åŠ è½½æ–‡æ¡£ä»: {folder}")
                    self.load_documents_from_folder(folder)
                    continue
                elif user_input.lower() == 'stats':
                    doc_count = self.collection.count()
                    print(f"æ–‡æ¡£å—æ•°é‡: {doc_count}")
                    print(f"åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
                    print(f"LLMæ¨¡å‹: {os.path.basename(self.model_path)}")
                    continue
                elif not user_input:
                    continue
                
                # ç”Ÿæˆå›ç­”
                result = self.generate_response(user_input)
                
                print(f"\nğŸ¤– AFSIMåŠ©æ‰‹:")
                print("-"*60)
                print(result["response"])
                print("-"*60)
                if result["sources"]:
                    print("å‚è€ƒæ¥æº:")
                    for source in result["sources"][:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªæ¥æº
                        print(f"  â€¢ {source}")
                print("="*60)
                
            except KeyboardInterrupt:
                print("\nç¨‹åºå·²ä¸­æ–­")
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                traceback.print_exc()
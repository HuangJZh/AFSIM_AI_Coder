import os
import glob
import re
import logging
from typing import Dict, List, Set, Tuple
from pathlib import Path
import json
from utils import FileReader, ConfigManager

class AFSIMProjectLearner:
    def __init__(self, project_root: str):
        self.project_root = project_root
        self.config = ConfigManager()
        self.base_libraries = self.config.get('project.base_libraries', ["base_types", "base_types_nx"])
        self.code_folders = []
        self.all_files = {}
        self.import_dependencies = {}
        self.file_categories = {}
        self.logger = logging.getLogger(__name__)

    def analyze_project_structure(self):
        """åˆ†æé¡¹ç›®ç»“æ„"""
        self.logger.info("å¼€å§‹åˆ†æAFSIMé¡¹ç›®ç»“æ„...")

        if not os.path.exists(self.project_root):
            raise ValueError(f"é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {self.project_root}")

        # è·å–æ‰€æœ‰æ–‡ä»¶å¤¹
        all_folders = [f.name for f in os.scandir(self.project_root) if f.is_dir()]

        # åˆ†ç¦»åŸºç¡€åº“å’Œä»£ç æ–‡ä»¶å¤¹
        self.code_folders = [f for f in all_folders if f not in self.base_libraries]

        self.logger.info(f"å‘ç°åŸºç¡€åº“: {self.base_libraries}")
        self.logger.info(f"å‘ç°ä»£ç æ–‡ä»¶å¤¹: {len(self.code_folders)} ä¸ª")

        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
        self._collect_all_files()

        # åˆ†ææ–‡ä»¶åˆ†ç±»
        self._categorize_files()

        # åˆ†æå¯¼å…¥ä¾èµ–
        self._analyze_imports()

        self.logger.info(f"é¡¹ç›®åˆ†æå®Œæˆ: æ€»å…± {len(self.all_files)} ä¸ªæ–‡ä»¶")

    def _collect_all_files(self):
        """æ”¶é›†æ‰€æœ‰æ–‡ä»¶å†…å®¹ï¼ˆæ”¯æŒ .txt ä»£ç å’Œ .md æ•™ç¨‹ï¼‰"""
        all_folders = self.base_libraries + self.code_folders
        skipped_files = 0
        processed_files = 0

        # === 1. åŸæœ‰é€»è¾‘ï¼šéå†ç‰¹å®šæ–‡ä»¶å¤¹ä¸‹çš„ .txt ä»£ç æ–‡ä»¶ ===
        for folder in all_folders:
            folder_path = os.path.join(self.project_root, folder)
            if not os.path.exists(folder_path):
                self.logger.warning(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
                continue

            # æŸ¥æ‰¾æ‰€æœ‰.txtæ–‡ä»¶
            txt_files = glob.glob(os.path.join(folder_path, "**", "*.txt"), recursive=True)
            self._process_file_list(txt_files, folder, processed_files, skipped_files)

        # === 2. æ–°å¢é€»è¾‘ï¼šéå†æ•´ä¸ªé¡¹ç›®ä¸‹çš„ .md æ•™ç¨‹æ–‡ä»¶ ===
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾mdæ–‡ä»¶å¯èƒ½åœ¨æ ¹ç›®å½•æˆ–ä»»ä½•å­ç›®å½•
        md_files = glob.glob(os.path.join(self.project_root, "**", "*.md"), recursive=True)
        if md_files:
            self.logger.info(f"å‘ç° {len(md_files)} ä¸ªMarkdownæ•™ç¨‹æ–‡ä»¶")
            # å°† md æ–‡ä»¶è§†ä¸º "documentation" æ–‡ä»¶å¤¹ç±»åˆ«ï¼Œæˆ–è€…ä¿æŒå…¶æ‰€å±æ–‡ä»¶å¤¹
            self._process_file_list(md_files, "documentation", processed_files, skipped_files)

    def _process_file_list(self, file_list, folder_category, processed_count, skipped_count):
        """è¾…åŠ©å‡½æ•°ï¼šå¤„ç†æ–‡ä»¶åˆ—è¡¨è¯»å–"""
        # æ³¨æ„ï¼šç”±äºæ•´æ•°æ˜¯ä¸å¯å˜ç±»å‹ï¼Œè¿™é‡Œåªæ˜¯ç®€å•å¤„ç†é€»è¾‘ï¼Œå®é™…è®¡æ•°åœ¨self.all_filesé•¿åº¦ä¸­ä½“ç°å³å¯
        for file_path in file_list:
            if FileReader.should_skip_file(file_path):
                continue

            relative_path = os.path.relpath(file_path, self.project_root)
            
            # é¿å…é‡å¤å¤„ç†ï¼ˆå¦‚æœ.mdæ–‡ä»¶åˆšå¥½åœ¨ä»£ç æ–‡ä»¶å¤¹é‡Œï¼‰
            if relative_path in self.all_files:
                continue

            try:
                content, encoding = FileReader.read_file_safely(file_path)
                if content.strip():  # åªä¿å­˜éç©ºæ–‡ä»¶
                    self.all_files[relative_path] = {
                        'content': content,
                        'size': len(content),
                        'folder': folder_category, # å¯¹äºMDæ–‡ä»¶ï¼Œè¿™é‡Œè¢«æ ‡è®°ä¸ºdocumentationæˆ–æ‰€å±æ–‡ä»¶å¤¹
                        'encoding': encoding,
                        'extension': os.path.splitext(file_path)[1].lower() # è®°å½•æ‰©å±•å
                    }
                else:
                    self.logger.debug(f"è·³è¿‡ç©ºæ–‡ä»¶: {file_path}")
            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

    def _categorize_files(self):
        """å¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç±» - åŒ…å«æ•™ç¨‹åˆ†ç±»"""
        categories = {
            'platforms': [],
            'weapons': [],
            'sensors': [],
            'processors': [],
            'scenarios': [],
            'behaviors': [],
            'signatures': [],
            'scripts': [],
            'tutorials': [], # === æ–°å¢ï¼šæ•™ç¨‹åˆ†ç±» ===
            'other': []
        }

        for file_path, file_info in self.all_files.items():
            # content = file_info['content'].lower() # æš‚æ—¶æ²¡ç”¨åˆ°å†…å®¹åˆ†ç±»ï¼ŒèŠ‚çœæ€§èƒ½
            
            # é¦–å…ˆåŸºäºè·¯å¾„è¿›è¡Œåˆ†ç±»
            self._categorize_by_path(file_path, categories)

        self.file_categories = categories

        # è®°å½•åˆ†ç±»ç»Ÿè®¡
        self.logger.info("æ–‡ä»¶åˆ†ç±»ç»Ÿè®¡:")
        for category, files in categories.items():
            self.logger.info(f"  {category}: {len(files)} ä¸ªæ–‡ä»¶")

    def _categorize_by_path(self, file_path: str, categories: dict) -> bool:
        """åŸºäºæ–‡ä»¶è·¯å¾„è¿›è¡Œåˆ†ç±»"""
        file_path_lower = file_path.lower()

        # === æ–°å¢ï¼šä¼˜å…ˆæ£€æŸ¥ Markdown æ–‡ä»¶ ===
        if file_path_lower.endswith('.md'):
            categories['tutorials'].append(file_path)
            return True

        # åŸæœ‰çš„è·¯å¾„å…³é”®è¯æ£€æŸ¥
        if 'platform' in file_path_lower and 'processor' not in file_path_lower:
            categories['platforms'].append(file_path)
            return True
        elif 'weapon' in file_path_lower:
            categories['weapons'].append(file_path)
            return True
        elif 'sensor' in file_path_lower:
            categories['sensors'].append(file_path)
            return True
        elif 'processor' in file_path_lower:
            categories['processors'].append(file_path)
            return True
        elif 'scenario' in file_path_lower:
            categories['scenarios'].append(file_path)
            return True
        elif 'behavior' in file_path_lower:
            categories['behaviors'].append(file_path)
            return True
        elif 'signature' in file_path_lower:
            categories['signatures'].append(file_path)
            return True
        elif 'script' in file_path_lower:
            categories['scripts'].append(file_path)
            return True
        
        # å…œåº•å½’ç±»
        categories['other'].append(file_path)
        return False

    def _analyze_imports(self):
        """åˆ†ææ–‡ä»¶é—´çš„å¯¼å…¥å…³ç³»ï¼ˆä»…é’ˆå¯¹ .txt ä»£ç æ–‡ä»¶ï¼‰"""
        for file_path, file_info in self.all_files.items():
            # === ä¿®æ”¹ï¼šè·³è¿‡ .md æ–‡ä»¶ï¼Œå®ƒä»¬æ²¡æœ‰ include è¯­æ³• ===
            if file_path.lower().endswith('.md'):
                continue
                
            content = file_info['content']
            imports = self._extract_imports(content)
            if imports:
                self.import_dependencies[file_path] = imports

    def _extract_imports(self, content: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–å¯¼å…¥è¯­å¥"""
        imports = []
        content = self._remove_cui_header(content)
        lines = content.split('\n')

        for line in lines:
            line = line.strip()
            if any(keyword in line.lower() for keyword in ['include', 'import', 'from', 'require', 'using']):
                line = re.sub(r'#.*$', '', line)
                line = re.sub(r'//.*$', '', line)
                line = re.sub(r'/\*.*?\*/', '', line)
                line = line.strip()
                if line:
                    imports.append(line)
        return imports

    def _remove_cui_header(self, content: str) -> str:
        """ç§»é™¤AFSIM CUIç‰ˆæƒå£°æ˜å¤´éƒ¨"""
        header_patterns = [
            r'# \*+\s*CUI\s*\*+.*?# \*+',
            r'# \*+\s*The Advanced Framework for Simulation.*?and LICENSE for details.*?# \*+',
        ]

        for pattern in header_patterns:
            content = re.sub(pattern, '', content, count=1, flags=re.DOTALL | re.IGNORECASE)

        cui_header = re.compile(
            r'^\s*# \*+\s*\n\s*# CUI\s*\n\s*# \*+\s*\n.*?The Advanced Framework for Simulation.*?and LICENSE for details.*?# \*+\s*',
            re.DOTALL | re.IGNORECASE | re.MULTILINE
        )
        content = cui_header.sub('', content, count=1)
        return content.strip()

    def get_file_content(self, relative_path: str) -> str:
        """è·å–æ–‡ä»¶å†…å®¹"""
        file_info = self.all_files.get(relative_path)
        return file_info['content'] if file_info else ""

    def find_related_files(self, query: str, max_results: int = 10) -> List[str]:
        """æŸ¥æ‰¾ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡ä»¶"""
        related_files = []
        query_lower = query.lower()
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        for file_path, file_info in self.all_files.items():
            content = file_info['content'].lower()
            if query_lower in content:
                related_files.append(file_path)

        return related_files[:max_results]

    def generate_context_prompt(self, query: str) -> str:
        """ç”ŸæˆåŒ…å«é¡¹ç›®ä¸Šä¸‹æ–‡çš„æç¤ºè¯ - å¢å¼ºç‰ˆï¼ˆåŒ…å«æ•™ç¨‹ï¼‰"""
        context_parts = []

        # 1. æ·»åŠ é¡¹ç›®ç»“æ„ä¿¡æ¯
        context_parts.append("=== AFSIMé¡¹ç›®ç»“æ„ ===")
        context_parts.append(f"åŸºç¡€åº“: {', '.join(self.base_libraries)}")
        context_parts.append(f"ä»£ç æ¨¡å—: {len(self.code_folders)} ä¸ª")
        
        # æ˜¾ç¤ºæ•™ç¨‹æ•°é‡
        tutorial_count = len(self.file_categories.get('tutorials', []))
        if tutorial_count > 0:
            context_parts.append(f"å¯ç”¨æ•™ç¨‹æ–‡æ¡£: {tutorial_count} ç¯‡")
        context_parts.append("")

        # 2. æŸ¥æ‰¾ç›¸å…³æ–‡ä»¶ï¼ˆæ··åˆä»£ç å’Œæ•™ç¨‹ï¼‰
        related_files = self.find_related_files(query, 8) # ç¨å¾®å¢åŠ æ•°é‡ä»¥å®¹çº³æ•™ç¨‹
        
        related_code = []
        related_tutorials = []

        if related_files:
            for f in related_files:
                if f.endswith('.md'):
                    related_tutorials.append(f)
                else:
                    related_code.append(f)

        # 3. ä¼˜å…ˆå±•ç¤ºç›¸å…³æ•™ç¨‹ï¼ˆæ¦‚å¿µä¼˜å…ˆï¼‰
        if related_tutorials:
            context_parts.append("=== ğŸ“š ç›¸å…³æ•™ç¨‹æ–‡æ¡£ ===")
            context_parts.append("ä»¥ä¸‹æ˜¯å¯èƒ½åŒ…å«ç›¸å…³æ¦‚å¿µè§£é‡Šçš„æ•™ç¨‹æ–‡ä»¶ï¼š")
            for file_path in related_tutorials:
                # è·å–æ•™ç¨‹å†…å®¹çš„å‰500ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦ï¼Œæˆ–è€…æ ¹æ®æŸ¥è¯¢è¯æˆªå–ç‰‡æ®µ
                content = self.get_file_content(file_path)
                preview = content[:500].replace('\n', ' ') + "..."
                context_parts.append(f"- æ–‡ä»¶: {file_path}")
                context_parts.append(f"  æ‘˜è¦: {preview}\n")
            context_parts.append("")

        # 4. å±•ç¤ºç›¸å…³ä»£ç æ–‡ä»¶
        if related_code:
            context_parts.append("=== ğŸ’» ç›¸å…³ä»£ç æ–‡ä»¶ ===")
            for file_path in related_code:
                context_parts.append(f"- {file_path}")
            context_parts.append("")
        
        if not related_code and not related_tutorials:
            context_parts.append("(æœªæ‰¾åˆ°ç›´æ¥åŒ…å«æŸ¥è¯¢å…³é”®è¯çš„æ–‡ä»¶ï¼Œå°†åŸºäºé€šç”¨çŸ¥è¯†å›ç­”)")
            context_parts.append("")

        # 5. æ·»åŠ æ–‡ä»¶åˆ†ç±»æ¦‚è§ˆ
        context_parts.append("=== æ–‡ä»¶ç±»å‹åˆ†å¸ƒ ===")
        for category, files in self.file_categories.items():
            if files and category != 'tutorials': # æ•™ç¨‹å·²åœ¨ä¸Šé¢å•ç‹¬å¤„ç†
                context_parts.append(f"{category}: {len(files)} ä¸ªæ–‡ä»¶")
        context_parts.append("")

        # 6. æ·»åŠ åŸºç¡€åº“çš„å…³é”®å†…å®¹ç¤ºä¾‹ (ä»…åœ¨æ²¡æœ‰å…·ä½“æ•™ç¨‹æ—¶è¯¦ç»†å±•ç¤ºï¼ŒèŠ‚çœä¸Šä¸‹æ–‡)
        if not related_tutorials:
            context_parts.append("=== åŸºç¡€åº“æ¦‚è§ˆ ===")
            for base_lib in self.base_libraries:
                lib_files = [f for f in self.all_files.keys() if f.startswith(base_lib)]
                if lib_files:
                    platform_files = [f for f in lib_files if 'platform' in f.lower()]
                    context_parts.append(f"{base_lib} åŒ…å«: å¹³å°å®šä¹‰({len(platform_files)}), æ­¦å™¨, ä¼ æ„Ÿå™¨ç­‰ã€‚")

        return "\n".join(context_parts)

    def get_project_summary(self) -> Dict:
        """è·å–é¡¹ç›®æ‘˜è¦"""
        return {
            "total_files": len(self.all_files),
            "base_libraries": self.base_libraries,
            "code_modules": self.code_folders,
            "file_categories": {k: len(v) for k, v in self.file_categories.items()},
            "total_size": sum(info['size'] for info in self.all_files.values())
        }

    def save_analysis_report(self, output_path: str):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        report = {
            "project_summary": self.get_project_summary(),
            "file_categories_details": self.file_categories,
            "import_dependencies": self.import_dependencies
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        self.logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")